\chapter{Introduction}

\emph{When are two mathematical proofs the same?}

This fundamental question, sometimes known as \emph{Hilbert's $24^{th}$} problem \cite{Thie:03:Hilberts:yu}, is still open and it is the topic of active research. One approach to solving this problem is summarised by the following slogan, which I will use to explain the context of this thesis:

\[
\mbox{Locality}\qquad\rightarrow\qquad\mbox{Geometry}\qquad\rightarrow\qquad\mbox{Semantics}
\]

Semantics of proofs are, broadly speaking, either \emph{abstract}, like categorical axiomatisations \cite{}; or \emph{concrete}, like proof nets \cite{Gira:87:Linear-L:wm}. The two approaches are often tightly connected, but in this thesis we shall focus on the latter.

Proofs are usually represented as lists, trees or directed graphs of strings. These syntactic objects contain a lot of \emph{bureaucracy}, in the sense (due to Girard) that there are several representations of proofs that only differ for `inessential details'. Where `inessential details' are defined in terms of certain kinds of local permutations of inference rules. By considering proofs modulo this kind of bureaucracy, geometric shapes representing the essence of the proofs can be obtained. Analogously, we can consider permutations as continuous maps between proofs and the geometric shapes as topological invariants.

It is clear that our geometric models depend on what we considered to be `inessential differences', which in terms depend on what permutations our syntax allows us. In particular; the more flexible the syntax, the more inessential differences we can observe. Proof nets were born out of \emph{linear logic}, where the flexibility was provided by \emph{linear} inference rules, \emph{i.e.} inference rules which are restricted to only contain every subformula once in the premiss and once in the conclusion. Not every logic can be expressed in terms of linear inference rules alone, which motivates the generalisation of linearity to the concept of \emph{locality}. An inference rule is said to be local if it can be checked in constant time (assuming the number of atoms are fixed). Clearly, linear rules are local since there are no subformulae of unbounded size to compare. Furthermore, \emph{atomic} rules (\emph{i.e.}, rules that only operate on atoms) are local, since we can compare atoms in constant time. A consequence of limiting the complexity of checking rules is that it limits the `interdependency' of rule instances and, thus gives greater flexibility in manipulating proofs.

It turns out that \emph{classical logic}, our topic of interest, can be expressed in terms of only local rules. However, to acheive this, we are forced \cite{Brun:03:Two-Rest:mn} to leave the traditional realm of Gentzen's natural deduction and sequent calculus \cite{Gent:69:Investig:xi}, in favour of a more general formalism employing Guglielmi's deep-inference methodology \cite{Gugl:06:A-System:kl}. In summary; deep inference gives locality, locality gives geometry, and geometry gives semantics.

It should not be expected that a semantics of proofs will be able to answer the original question, \emph{When are two proofs the same?}, for all intents and purposes. Depending on the reason and context for asking the question, proofs might be `manipulated' in different ways before they are compared. This process of maniuplation is called \emph{normalisation}. A successfull semantics of proofs should not only give a criterion for when two proofs are `essentially the same', but clearly, it is also expected that two proofs that are `essentially the same' should yield `essentially the same' result after any kind of normalisation. So, in order to find the semantics of proofs, one must keep an eye on all possible ways of normalining proofs. Admittedly, no simple task.

The contribution of this thesis is a general view of normalisation through a geometric language we call \emph{atomic flows}.

As for natural deduction and the sequent calculus, we intend normalisation as eliminating cuts, or, more in general, `detours', from derivations. However, unlike in natural deduction or in the sequent calculus, derivations in deep-inference formalisms are symmetric in the premiss-conclusion axis, and `detours' turn out to be more than just the existence of cuts.

Atomic flows were motivated by wanting to generalise traditional normalisation, based on elimination of inference rules, to a new notion of normalisation based on elimination of dependencies between inference rules. In order to formalise the notion of `detours', atomic flows were defined by discarding from derivations all but the information about casual relations between creation and destruction of atoms (see Figure~\vref{figure:ExampleAtomicFlows}).

We claim that atomic flows give us a more general view of normalisation because they gives us new normal forms, of which the traditional normal forms are special cases; at the same time they show that we need less of the information available to us in order to obtain normalisation.

Due to the top-down symmetry, and the generalisation of `detours', atomic flows allow us to describe a range of new normal forms. In the special cases where `cut elimination' makes sense, the new normal forms coincide with the traditional ones, as expected.

Furthermore, it turned out that the information contained in atomic flows is sufficient to design normalisation procedures. This is surprising, since atomic flows throw away all information about logical connectives and logical inference rules, exactly the information we expected to be most crucial in order to obtain normalisation. The locality provided by deep inference is what makes this simplification possible.

In fact, the principal advantage of atomic flows is that it allows us to use the graphical language of atomic flows to describe the gist of our results. Once the language is mastered, the technical details of most of our results can easily be reconstructed based on the illustrations alone. Illustrations describing our normalisation procedures are given in Definition~\vref{definition:IsolatedSubflowRemoval}, Definition~\vref{definition:PathBreaker}, Definition~\vref{definition:MultipleIsolatedSubflowsRemoval} and Figure~\vref{figure:ReductionRules}.

The thesis is split in three parts. In the first part I introduce derivations in classical propositional logic and show some elementary results. In the second part I define atomic flows, show how flows are extracted from derivations and define normal forms of derivations in terms of their associated atomic flows. Finally, in the last part I define several normalisation procedures based on rewriting atomic flows.
