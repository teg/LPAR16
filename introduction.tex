\chapter{Introduction}

\emph{Proof theory} studies the formal representation of proofs. Formal representation of proofs are needed in order to automatically verify proofs, search for new proofs, prove consistency of theories, \emph{etc}.

Furthermore, representation of proofs are interesting mathematical objects in their own right. In particular, they are related to longstanding open problems. The problem of $NP$ versus $co$-$NP$, from computational complexity theory, and the problem of \emph{identity of proofs}, can both be stated in terms of representations of proofs.

The problem of identity of proofs was originally intended by Hilbert to be the 24th problem of his famous Paris lecture in 1900 \cite{Thie:03:Hilberts:yu}, and it amounts to answering the question: can be expressed as \emph{When are two proofs the same?} This thesis is part of a program which tries to solve the identity of proofs problem in the setting of propositional logic.

A language for representing proofs is called a \emph{formalism}, and the rules by which a proof is constructed are called \emph{inference rules}.

Formal proofs are not very good at conveying the `essence', `meaning' or \emph{semantics} of a proof. In fact, they are usually extremely verbose. Finding the semantics of proofs is still an open problem, closely related to a question older than proof theory itself:

It should not be expected that a semantics of proofs will be able to answer the above question for all intents and purposes. Depending on the reason and context for asking the question, proofs might be manipulated in different ways before their semantics are compared. This process of manipulation is called \emph{normalisation}.

% 'semantics' used to be: 'bureaucracy-free essence'

This thesis is part of a program whose aim it is to answer Hilbert's question. The main ideas of this program are: 1) proofs are essentially geometric objects; 2) their shape is their semantics; 3) normalisation manipulates the shape of proofs under geometric invariants. I will now outline the project and where this thesis fits into it.

\emph{Bureaucracy} is the name we give to `inessential details' in the representation of a proof. There is strong evidence that bureaucracy in proofs is what obscures their meaning and eliminating bureaucracy is what will allow us to discover their geometric essence. We have an instance of the kind of bureaucracy we are interested in, if independent parts of a proof are represented as if there is a dependency between them. A consequence of bureaucracy is that proofs that are essentially the same are given seemingly different representations.

% linear sequent calculus = sequent calculus without weakening and contraction

We are influenced by Girard's \emph{linear logic} and \emph{proof nets} \cite{Gira:87:Linear-L:wm}. Linear logic is, roughly speaking, a restriction of classical logic by only allowing \emph{linear} inference rules, \emph{i.e.}, rules that do not duplicate or destroy formulae. Proof nets are geometric representations of linear logic proofs, which identify proofs modulo bureaucracy.

In the same way that linearity gave proof nets in the case of linear logic, we want to find a geometric representation of proofs independent of the logic. However, not all logics can be represented with only linear rules. This motivated a generalisation of linearity to \emph{locality}. An inference rule is said to be local if the time needed to verify an instance of the rule is bounded by a constant (under certain mild hypothesis).

\TODO{Good place for deep inference.}

In addition to linear rules, a second class of local rules are the \emph{atomic} ones. Atomic rules are restricted to only apply to atoms. Unlike linear rules, atomic rules can duplicate or destroy formulae, but not of unbounded size. Not all rules are local: In order to verify an instance of a rule that duplicates a formula, the two copies of the formula must be compared. Since the size of a formula is unbounded, the rule is not local.

Intuitively, since local rules can only depend on a limited amount of information, the `interdependence' of instances of local rules is limited. In terms of bureaucracy, local rules make it easier to recognise when sub-proofs are independent. \TODO{Why?}

\emph{Deep inference} \cite{Gugl:06:A-System:kl} is a methodology which allows for more general formalisms than Gentzen's traditional \emph{natural deduction} and \emph{sequent calculus} \cite{Gent:69:Investig:xi}. In particular, deep-inference formalisms can express classical logic using only local inference rules \cite{BrunTiu:01:A-Local-:mz}, something which is impossible in the traditional formalisms \cite{Brun:03:Two-Rest:mn}.


\TODO{reveals?}
In summary, deep inference makes locality possible; locality reveals bureaucracy; eliminating bureaucracy allows us to represent proofs as geometric objects; whose shape we conjecture will lead us to a nice semantics:
\[
\mbox{Locality}\qquad\rightarrow\qquad\mbox{Geometry}\qquad\rightarrow\qquad\mbox{Semantics}\quad.
\]

As mentioned above, proofs sometimes need to be normalised in order to be compared. The contribution of this thesis is a geometric language we call \emph{atomic flows} \cite{GuglGund:07:Normalis:lr}, which provides a general view of normalisation.

\TODO{Cut elimination}

As for natural deduction and the sequent calculus, we intend normalisation as eliminating \emph{cuts}, or more generally `detours', from proofs. However, unlike natural deduction and the sequent calculus, deep-inference formalisms allow a more general kind proofs, called \emph{derivations}. Unlike proofs, derivations are symmetric in the top-down axis. For this reason, `detours' turn out to be more than just the existence of cuts.

\TODO{Geometry}

Atomic flows were motivated by wanting to generalise traditional normalisation, based on elimination of inference rules, to a new notion of normalisation based on elimination of dependencies between inference rule instances. In order to formalise the notion of `detours', atomic flows were defined by discarding from derivations all but the information about causal relations between creation and destruction of atoms, \emph{i.e.}, the information contained in the atomic inference rules.

\TODO{First explain that logic goes into linear rules.}

We claim that atomic flows give a more general view of normalisation because they provide new normal forms, of which the traditional normal forms are special cases; at the same time they show that less of the available information is needed in order to obtain normalisation. \TODO{And surprisingly: not the logical relations!}

\TODO{No detours}

Due to the top-down symmetry, and the generalisation of `detours', atomic flows allow us to describe a range of new normal forms. In the special cases where `cut elimination' makes sense, the new normal forms coincide with the traditional ones.

Conventional wisdom teaches us that in order to achieve normalisation a certain \emph{harmony} between the linear inference rules is needed. Atomic flows discard all information about logical relations and linear rules, so we found it surprising that the information contained in atomic flows is sufficient to design normalisation procedures. The locality provided by deep inference is what makes this simplification possible.

\TODO{What about harmony? Talk about robustness}

Strictly speaking, our results could be expressed without the use of atomic flows. However, the principal advantage of atomic flows is that it allows us to use a graphical language to describe the gist of our results. Once the language is mastered, the technical details of most of our results can easily be reconstructed based on the illustrations alone. Furthermore, atomic flows provide an intuition for working with normalisation which greatly simplified the discovery of some of our results.

Some of the results in this thesis are also available in journal articles. One article has been published, one has been submitted and three are still beeing written. They are, respectively:
\begin{itemize}
 \item atomic flows were introduced and the first normalisation results were shown in \emph{Normalisation Control in Deep Inference via Atomic Flows} with Alessio Guglielmi, published in \emph{Logical Methods in Computer Science},
 \item quasipolynomial cut-elimination was shown using atomic flows in \emph{Quasipolynomial Normalisation in Deep Inference via Atomic Flows and Threshold Formulae}, with Paola Bruscoli, Alessio Guglielmi and Michel Parigot, submitted to \emph{Mathematical Structures in Computer Science},
 \item we are showing refined and generalised normalisation results in \emph{Normalisation Control in Deep Inference via Atomic Flows II}, with Alessio Guglielmi,
 \item we are showing (among other things) that there is a polynomial relationship between the sizes of derivations and atomic flows in \emph{On the Complexity of the Switch and Medial Rules}, with Paola Bruscoly, Alessio Guglielmi and Lutz Stra\ss{}burger, and
 \item we are defining two new deep-inference formalisms, for which atomic flows and the associated normalisation procedures are invariants in \emph{Formalism A and Formalism B}, with Alessio Guglielmi and Michel Parigot
\end{itemize}

In these three years we could see that many people saw something in our atomic flows. In particular, I have been involved in discussions about using atomic flows for computational interpretations, implicit computational complexity, as well as normalisation of intuitionistic, modal and first-order logics.

At the moment atomic flows are a tool for studying normalisation. However, the most promising direction of research on atomic flows for the future, is in my opinion, to define a new formalism based on atomic flows with the aim of solving the identity of proofs. Atomic flows are promising in this regard, since they are bureaucracy free, they represent how proofs behave under (a range of different notions of) normalisation, and they represent the size of proofs. However, an important open question is to decide how much information needs to be added to atomic flows before they preserve the validity of proofs.

The thesis is split in three parts.
\begin{itemize}
\item In the first part I introduce the deep-inference formalism we will be using and classical propositional logic.
\item In the second part I define atomic flows, show how flows are related to derivations and define normal forms of derivations in terms of their associated atomic flows.
\item Finally, in the last part I define several normalisation procedures based on rewriting atomic flows.
\end{itemize}